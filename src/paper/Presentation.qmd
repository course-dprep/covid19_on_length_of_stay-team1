---
title: "The impact of COVID-19 on the minimum nights of stay for Airbnb's in Amsterdam"
author: "Team 1"
format: 
  revealjs:
    footer: "COVID-19 on minimum nights of stay - Team 1"
    theme: default
    slide-number: true
    embed-resources: true
    controls: true
    transition: slide
    scrollable: true
    smaller: true
editor: visual
---

# Introduction and Motivation

## Introduction and Motivation

```{r, echo=FALSE}
# load packages
library(quarto)
library(readr)
library(dplyr)
library(broom)
library(ggplot2)
library(ggpubr)
df_cleaned <- read_csv("../../gen/data-preparation/output/data_no_outliers.csv")
```
For the project of the course Data Preparation and Workflow Management at Tilburg University, we decided to analyze the Airbnb market in Amsterdam and especially if the COVID-19 pandemic had an influence on the required minimum nights of stay. Especially since there are contradicted foundings in literature regarding this subject. A recent article by the New York Times suggested that the minimum nights of stay increased in New York City during the COVID-19 pandemic, whereas research by Kourtit et al. concluded that the minimum night requirements actually decreased during the pandemic. We decided to take a further look at these contradictions, by researching this subject. We collected data from Airbnb in Amsterdam, from 2020 as well as 2022, to see if there is any significant difference in the minimum nights of stay between *during* and *after* the COVID-19 pandemic.

# Research Method

## Research Method (1) {auto-animate="true"}

We decided to run a linear regression on the variables of interest. The dependent variable, the required minimum nights, is a metric variable and the independent variable, the presence of COVID-19 (present vs. absent) is a non-metric variable. We have data from 2020 and 2022 for 3960 different Airbnb listings (in total 7920 observations). The variable gets the value 1 assigned if the data is from 2020, so when there was COVID-19 in the Netherlands. Following from that, the variable gets the value 0 assigned if the data is from 2022, when the COVID-19 pandemic no longer had far-reaching consequences in the Netherlands.
We decided to not only include the minimum nights of stay and the presence of COVID-19, but also added some control variables to our analysis, to see if there are other effects that might play a role. Since these control variables are differing in metric and non-metric variables, we have chosen linear regression over an ANOVA-analysis.

## Research Method (2) {auto-animate="true"}

Next to the dependent variable, the *minimum_nights*, and the independent variable *covid*, we included some control variables in a first regression. The control variables *neighbourhood_num* and *roomtype_num* were converted to factors, in which each number represents a different neighbourhood or roomtype. Next to that, *accommodates*, *price* and *instant_bookable* were included in this regression.

```{r}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "2|3"
# Estimate simple model
m1 <- lm(minimum_nights ~ covid + as.factor(neighbourhood_num) + as.factor(roomtype_num) + accommodates + price + instant_bookable, df_cleaned)
summary(m1)
```

# Results

## Results

Following from the output from the first regression, we can conclude that a lot of the estimates are not significant in this model. Looking at the independent variable, *covid*, we can conclude that in case that *covid* was TRUE (observations assigned to the value 1, which means the data is from 2020), the minimum nights of stay decreased by 0.3726339. The fourth and 16th factor of the variable *neighbourhood_num* also seem to have a significant effect on the minimum nights of stay with the first factor of the variable *neighbourhood_num* as baseline. The fourth factor of the variable *neighbourhood_num* respresents the neighbourhood 'De Aker - Nieuw Sloten'and the 16th factor represents the neighbourhood 'Gaasperdam - Driemond'. Both neighbourhoods are not in the city center, so a possible explanation could be that these Airbnb listings are more focused on longer stays and a little less on tourists. Next to that, the second and third factor of the variable *roomtype_num* also seem to have an significant effect on the minimum nights of stay with the first factor of the variable *roomtype_num* as baseline. The second factor of the variable *roomtype_num* represents an entire home/apartment and the third factor represents a private room. Finally, the variables *price* and *instant_bookable* (when the value is TRUE), also seem to have a significant effect on the minimum nights of stay.represents a private room. Finally, the variables *price* and *instant_bookable* (when the value is TRUE), also seem to have a significant effect on the minimum nights of stay.

```{r}
summary(m1)
```

# Robustness Checks

Before any conclusions can be drawn from the regression, we need to perform some robustness checks.

## Independence (1) {auto-animate="true"}

```{r}
#| echo: true
#| output-location: fragment
# create a scatterplot of the residuals against the predicted values from the linear regression model 'm1'
plot(m1$fitted.values, m1$residuals, 
     xlab = "Fitted Values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values Plot",
     ylim = c(-50, 60))
abline(h = 0, lty = 2, col = 'red')
```

## Independence (1) {auto-animate="true"}

A first option to check for independence, is to create a scatterplot of the residuals against the fitted values from the linear regression model (in this case *m1*).
The residuals should be independent from the variable, but this scatterplot shows us that this is not the case. We can conclude that there is no independence of the residuals.

```{r}
#| echo: false
plot(m1$fitted.values, m1$residuals, 
     xlab = "Fitted Values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values Plot",
     ylim = c(-50, 60))
abline(h = 0, lty = 2, col = 'red')
```

## Independence (2) {auto-animate="true"}

```{r, echo=FALSE}
# Add predicted values to the data frame
df_cleaned$predicted <- predict(m1)
```

```{r}
#| echo: true
#| output-location: fragment
# Create a scatterplot of predicted vs actual values
ggplot(df_cleaned, aes(x = predicted, y = minimum_nights)) +
  geom_point() + # adds points to the plot
  geom_abline(intercept = 0, slope = 1, color = "red") + # adds a diagonal line to the plot to visualize where predicted = actual
  xlab("Predicted Values") + # adds a label for the x-axis
  ylab("Actual Values") + # adds a label for the y-axis
  ggtitle("Predicted vs Actual Values Plot") # adds a title to the plot
```

## Independence (2) {auto-animate="true"}

Next to that, we can create a scatterplot of the predicted values against the actual values. Looking at the scatterplot we notice that the dots are not randomly scattered. Although the plot does not show an obvious pattern it certainly does not represent a randomization of the dots since there are way more positive outliers than negative. 

```{r}
#| echo: false
# Create a scatterplot of predicted vs actual values
ggplot(df_cleaned, aes(x = predicted, y = minimum_nights)) +
  geom_point() + # adds points to the plot
  geom_abline(intercept = 0, slope = 1, color = "red") + # adds a diagonal line to the plot to visualize where predicted = actual
  xlab("Predicted Values") + # adds a label for the x-axis
  ylab("Actual Values") + # adds a label for the y-axis
  ggtitle("Predicted vs Actual Values Plot") # adds a title to the plot
```

## Independence (3) {auto-animate="true"}

``` r
# perform Durbin-Watson test
```

## Independence (3) {auto-animate="true"}

``` r
# perform Durbin-Watson test
dwtest(m1)
```

```{r, echo=FALSE}
# perform Durbin-Watson test
library(lmtest)
dwtest(m1)
```

## Independence (3) {auto-animate="true"}

``` r
# perform Durbin-Watson test
dwtest(m1)
```

```{r, echo=FALSE}
# perform Durbin-Watson test
library(lmtest)
dwtest(m1)
```

A last option to check for independence, is to use a Durbin-Watson test. The Durbin Watson statistic is a test statistic used in statistics to detect autocorrelation in the residuals from a regression analysis. If the value of the test is DW = 2, then there is no autocorrelation. With the Durbin-Watson test we achieved a p-value < 0 which means that we would reject this null hypothesis. Following from the output, the DW = 0.1109, which means that there is autocorrelation and thus no independence.

## Homoskedasticity (1)

We can use the same plots when checking for homoskedasticity, as for checking independence. The spread of the residuals is not consistent across all values of the predictor variables, so the homoskedasticity assumption is therefore not met.

```{r}
#| echo: false
plot(m1$fitted.values, m1$residuals, 
     xlab = "Fitted Values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values Plot",
     ylim = c(-50, 60))
abline(h = 0, lty = 2, col = 'red')
```

```{r}
ggplot(df_cleaned, aes(x = predicted, y = minimum_nights)) +
  geom_point() + # adds points to the plot
  geom_abline(intercept = 0, slope = 1, color = "red") + # adds a diagonal line to the plot to visualize where predicted = actual
  xlab("Predicted Values") + # adds a label for the x-axis
  ylab("Actual Values") + # adds a label for the y-axis
  ggtitle("Predicted vs Actual Values Plot") # adds a title to the plot
```

## Homoskedasticity (2) {auto-animate="true"}

``` r
# perform Breusch-Pagan test
```

## Homoskedasticity (2) {auto-animate="true"}

``` r
# perform Breusch-Pagan test
bptest(m1)
```

```{r, echo=FALSE}
# perform Breusch-Pagan test
bptest(m1)
```

## Homoskedasticity (2) {auto-animate="true"}

``` r
# perform Breusch-Pagan test
bptest(m1)
```

```{r, echo=FALSE}
# perform Breusch-Pagan test
bptest(m1)
```

To provide more evidence that this assumption is not met, we used the Breusch-Pagan test. It tests whether the variance of the errors from a regression is dependent on the values of the independent variables. Since the p-value of this test is < 0.05, we can conclude that the homoskedasticity assumption is not met.

## Normality (1) {auto-animate="true"}

```{r}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "1,2,3|1,2,3,4,5,6,7,8,9"
## Making a dataframe with the residuals
residuals <- resid(m1)
residuals_df <- data.frame(residuals = residuals)

# Test for normality of residuals with a histogram
ggplot(residuals_df, aes(x = residuals)) + 
  geom_histogram(binwidth = 0.5, color = "black", fill = "white") + 
  xlab("Residuals") + ylab("Frequency") +
  ggtitle("Histogram of Residuals")
```

## Normality (1) {auto-animate="true"}

Overall, we can conclude there is non-normality for the residuals of our regression. Up until a certain value, there is normality of residuals, but high outliers cause it to be non-normal. Since it are only the high extremes that are not normal, we take no measures against non-normality of our residuals.

```{r}
#| echo: false
ggplot(residuals_df, aes(x = residuals)) + 
  geom_histogram(binwidth = 0.5, color = "black", fill = "white") + 
  xlab("Residuals") + ylab("Frequency") +
  ggtitle("Histogram of Residuals")
```

## Normality (2) {auto-animate="true"}

```{r}
#| echo: true
#| output-location: fragment

# Test for normality of residuals with a density plot
ggdensity(residuals_df$residuals, 
          main = "Density plot of residuals",
          xlab = "residuals")
```

## Normality (2) {auto-animate="true"}

```{r}
#| echo: false
ggdensity(residuals_df$residuals, 
          main = "Density plot of residuals",
          xlab = "residuals")
```

## Normality (3) {auto-animate="true"}

```{r}
#| echo: true
#| output-location: fragment

# Test for normality with a Q-Q plot
qqnorm(residuals)
qqline(residuals)
```

## Normality (3) {auto-animate="true"}

```{r}
#| echo: false
qqnorm(residuals)
qqline(residuals)
```

## Normality (4)

```{r, echo=TRUE}
# Create random subsample of 5000 observations, so we are able to run a Shapiro-Wilk normality test (5000 is the maximum sample size)
set.seed(123)
my_subsample <- residuals_df[sample(nrow(residuals_df), 5000), ]
shapiro.test(my_subsample)
```

A last option to check for normality is performing a Shapiro-Wilk test. The Shapiro–Wilk test can be used to decide whether or not a sample fits a normal distribution. Since the p-value < 0.05, the data significantly deviate from a normal distribution.

## Linearity (1)

We can use the same first plot as used for testing independence and homoskedasticity. If the relationship between minimum_nights and the independent variables is linear, then the points in the scatterplot should be randomly scattered around a horizontal line at zero, indicating that the residuals are distributed randomly and evenly across the range of predicted minimum night values. However, if we look at our plot, this is not the case since the points in the scatterplot are not randomly scattered around the horizontal line zero. In our case we have a lot of positive outliers and almost no negative residuals. This indicates that there is no linearity.

```{r}
#| echo: false
plot(m1$fitted.values, m1$residuals, 
     xlab = "Fitted Values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values Plot",
     ylim = c(-50, 60))
abline(h = 0, lty = 2, col = 'red')
```

## Multicollinearity (1)

```{r}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "1,2|1,2,3"
# VIF test 
library(car)
vif(m1)
```

An option to check for multicollinearity is calculating VIFs. Looking at the VIF values we can conclude that there is no significant multicollinearity among the predictor variables. All the VIF values are around 1, where generally VIF values between 5 and 10 indicate significant multicollinearity.

## Multicollinearity (2) {auto-animate="true"}

``` r
# correlation matrix
```

## Multicollinearity (2) {auto-animate="true"}

``` r
# correlation matrix
cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")])
```

```{r, echo=FALSE}
cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")])
```

## Multicollinearity (2) {auto-animate="true"}

``` r
# correlation matrix
cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")])
```

```{r, echo=FALSE}
cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")])
```
Another option to check for multicollinearity is creating a correlation matrix. Given the correlation coefficients in the matrix, none of them is > 0.7. The highest correlation coefficient is 0.51640989, between the variables *price* and *accommodates*.

## Multicollinearity (3) {auto-animate="true"}

``` r
# eigenvalues and condition number 
```

## Multicollinearity (3) {auto-animate="true"}

``` r
# eigenvalues and condition number 
eigen(cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")]))$values
kappa(model.matrix(m1))
```

```{r, echo=FALSE}
eigen(cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")]))$values
kappa(model.matrix(m1))
```

## Multicollinearity (3) {auto-animate="true"}

``` r
# eigenvalues and condition number 
eigen(cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")]))$values
kappa(model.matrix(m1))
```

```{r, echo=FALSE}
eigen(cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")]))$values
kappa(model.matrix(m1))
```

Deze slide(s) moeten weg

# Conclusion

## Conclusion

Since the assumptions are not met, it is hard to draw decent conclusions about the linear regression. Further research could look into the data and try to meet the assumptions to eventually draw some meaningful conclusions. Another way is to conduct an ANOVA-analysis, leaving some of the control variables out and see if these lead to meaningful conclusions about the data.
