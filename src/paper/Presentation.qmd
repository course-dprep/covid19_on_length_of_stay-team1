---
title: "The impact of covid 19 on the minimum nights of stay for airbnb's in Amsterdam"
author: "Team 1"
format: 
  revealjs:
    footer: "covid 19 on minimum nights of stay - team 1"
    theme: default
    slide-number: true
    embed-resources: true
    controls: true
    transition: slide
    scrollable: true
editor: visual
---

# Introduction

## Introduction

```{r, echo=FALSE}
# load packages
library(quarto)
library(readr)
library(dplyr)
library(broom)
library(ggplot2)
library(ggpubr)
df_cleaned <- read_csv("../../gen/data-preparation/output/data_no_outliers.csv")
```
For the project of the course Data Preparation and Workflow Management at Tilburg University, we decided to analyze the Airbnb market in Amsterdam and especially if the COVID-19 pandemic had an influence on the required minimum nights of stay. Especially since there are contradicted foundings in literature regarding this subject. A recent article by the New York Times suggested that the minimum nights of stay increased in New York City during the COVID-19 pandemic, whereas research by Kourtit et al. concluded that the minimum night requirements actually decreased during the pandemic. We decided to take a further look at these contradictions, by researching this subject. We collected data from Airbnb in Amsterdam, from 2020 as well as 2022, to see if there is any significant difference in the minimum nights of stay between *during* and *after* the COVID-19 pandemic.

# Motivation

## Motivation
Samengevoegd met Intro

# Research Method

## Research Method (1) {auto-animate="true"}

We decided to run a linear regression on the variables of interest. The dependent variable, the required minimum nights, is a metric variable and the independent variable, the presence of COVID-19 (present vs. absent) is a non-metric variable. We have data from 2020 and 2022 for 3960 different Airbnb listings (in total 7920 observations). The variable gets the value 1 assigned if the data is from 2020, so when there was COVID-19 in the Netherlands. Following from that, the variable gets the value 0 assigned if the data is from 2022, when the COVID-19 pandemic no longer had far-reaching consequences in the Netherlands.
We decided to not only include the minimum nights of stay and the presence of COVID-19, but also added some control variables to our analysis, to see if there are other effects that might play a role. Since these control variables are differing in metric and non-metric variables, we have chosen linear regression over an ANOVA-analysis.

## Research method (2) {auto-animate="true"}

Next to the dependent variable, the *minimum_nights*, and the independent variable *covid*, we included some control variables in a first regression. The control variables *neighbourhood_num* and *roomtype_num* were converted to factors, in which each number represents a different neighbourhood or roomtype. Next to that, *accomodates*, *price* and *instant_bookable* were included in this regression.

```{r}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "2|3"
# Estimate simple model
m1 <- lm(minimum_nights ~ covid + as.factor(neighbourhood_num) + as.factor(roomtype_num) + accommodates + price + instant_bookable, df_cleaned)
summary(m1)
```

# Results

## Results

Following from the output from the first regression, we can conclude that a lot of the estimates are not significant in this model. 

```{r}
summary(m1)
```

# Robustness Checks

Before any conclusions can be drawn, we need to perform some robustness checks.

## Independence (1) {auto-animate="true"}

```{r}
#| echo: true
#| output-location: fragment
# create a scatterplot of the residuals against the predicted values from the linear regression model 'm1'
plot(m1$fitted.values, m1$residuals, 
     xlab = "Fitted Values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values Plot",
     ylim = c(-50, 60))
abline(h = 0, lty = 2, col = 'red')
```

## Independence (1) {auto-animate="true"}

A first option to check for independence, is to create a scatterplot of the residuals against the fitted values from the linear regression model (in this case m1).
The residuals should be independent from the variable, but this scatterplot shows us that this is not the case. We can conclude that there is no independence of the residuals.

```{r}
#| echo: false
plot(m1$fitted.values, m1$residuals, 
     xlab = "Fitted Values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values Plot",
     ylim = c(-50, 60))
abline(h = 0, lty = 2, col = 'red')
```

## Independence (2) {auto-animate="true"}

```{r, echo=FALSE}
# Add predicted values to the data frame
df_cleaned$predicted <- predict(m1)
```

```{r}
#| echo: true
#| output-location: fragment
# Create a scatterplot of predicted vs actual values
ggplot(df_cleaned, aes(x = predicted, y = minimum_nights)) +
  geom_point() + # adds points to the plot
  geom_abline(intercept = 0, slope = 1, color = "red") + # adds a diagonal line to the plot to visualize where predicted = actual
  xlab("Predicted Values") + # adds a label for the x-axis
  ylab("Actual Values") + # adds a label for the y-axis
  ggtitle("Predicted vs Actual Values Plot") # adds a title to the plot
```

## Independence (2) {auto-animate="true"}

Next to that, we can create a scatterplot of the predicted values against the actual values. 

```{r}
#| echo: false
# Create a scatterplot of predicted vs actual values
ggplot(df_cleaned, aes(x = predicted, y = minimum_nights)) +
  geom_point() + # adds points to the plot
  geom_abline(intercept = 0, slope = 1, color = "red") + # adds a diagonal line to the plot to visualize where predicted = actual
  xlab("Predicted Values") + # adds a label for the x-axis
  ylab("Actual Values") + # adds a label for the y-axis
  ggtitle("Predicted vs Actual Values Plot") # adds a title to the plot
```

## Independence (3) {auto-animate="true"}

``` r
# perform Durbin-Watson test
```

## Independence (3) {auto-animate="true"}

``` r
# perform Durbin-Watson test
dwtest(m1)
```

```{r, echo=FALSE}
# perform Durbin-Watson test
library(lmtest)
dwtest(m1)
```

## Independence (3) {auto-animate="true"}

``` r
# perform Durbin-Watson test
dwtest(m1)
```

```{r, echo=FALSE}
# perform Durbin-Watson test
library(lmtest)
dwtest(m1)
```

TYPE HIER: a last thing we can do to check for independence is performing a Durbin-Watson test...

## Homoskedasticity (1)

We can use the same plots when checking for homoskedasticity, as for checking independence.

```{r}
#| echo: false
plot(m1$fitted.values, m1$residuals, 
     xlab = "Fitted Values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values Plot",
     ylim = c(-50, 60))
abline(h = 0, lty = 2, col = 'red')
```

```{r}
ggplot(df_cleaned, aes(x = predicted, y = minimum_nights)) +
  geom_point() + # adds points to the plot
  geom_abline(intercept = 0, slope = 1, color = "red") + # adds a diagonal line to the plot to visualize where predicted = actual
  xlab("Predicted Values") + # adds a label for the x-axis
  ylab("Actual Values") + # adds a label for the y-axis
  ggtitle("Predicted vs Actual Values Plot") # adds a title to the plot
```

## Homoskedasticity (2) {auto-animate="true"}

``` r
# perform Breusch-Pagan test
```

## Homoskedasticity (2) {auto-animate="true"}

``` r
# perform Breusch-Pagan test
bptest(m1)
```

```{r, echo=FALSE}
# perform Breusch-Pagan test
bptest(m1)
```

## Homoskedasticity (2) {auto-animate="true"}

``` r
# perform Breusch-Pagan test
bptest(m1)
```

```{r, echo=FALSE}
# perform Breusch-Pagan test
bptest(m1)
```

TYPE HIER: a last thing we can do to check for homoskedasticity is performing a Breusch-Pagan test...

## Normality (1) {auto-animate="true"}

```{r}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "1,2,3|1,2,3,4,5,6,7,8,9"
## Making a dataframe with the residuals
residuals <- resid(m1)
residuals_df <- data.frame(residuals = residuals)

# Test for normality of residuals with a histogram
ggplot(residuals_df, aes(x = residuals)) + 
  geom_histogram(binwidth = 0.5, color = "black", fill = "white") + 
  xlab("Residuals") + ylab("Frequency") +
  ggtitle("Histogram of Residuals")
```

## Normality (1) {auto-animate="true"}

TYPE HIER: Type hier waarom de graph laat zien dat er geen independence is

```{r}
#| echo: false
ggplot(residuals_df, aes(x = residuals)) + 
  geom_histogram(binwidth = 0.5, color = "black", fill = "white") + 
  xlab("Residuals") + ylab("Frequency") +
  ggtitle("Histogram of Residuals")
```

## Normality (2) {auto-animate="true"}

```{r}
#| echo: true
#| output-location: fragment

# Test for normality of residuals with a density plot
ggdensity(residuals_df$residuals, 
          main = "Density plot of residuals",
          xlab = "residuals")
```

## Normality (2) {auto-animate="true"}

TYPE HIER: Type hier waarom de graph laat zien dat er geen independence is

```{r}
#| echo: false
ggdensity(residuals_df$residuals, 
          main = "Density plot of residuals",
          xlab = "residuals")
```

## Normality (3) {auto-animate="true"}

```{r}
#| echo: true
#| output-location: fragment

# Test for normality with a Q-Q plot
qqnorm(residuals)
qqline(residuals)
```

## Normality (3) {auto-animate="true"}

TYPE HIER: Type hier waarom de graph laat zien dat er geen independence is

```{r}
#| echo: false
qqnorm(residuals)
qqline(residuals)
```

## Normality (4)

```{r, echo=TRUE}
# Create random subsample of 5000 observations, so we are able to run a Shapiro-Wilk normality test (5000 is the maximum sample size)
set.seed(123)
my_subsample <- residuals_df[sample(nrow(residuals_df), 5000), ]
shapiro.test(my_subsample)
```

TYPE HIER: a last thing we can do to check for normality is performing a Shapiro-Wilk normality test...

## Linearity (1)

We can use the same first plot as used for testing independence and homoskedasticity. We can conclude...

```{r}
#| echo: false
plot(m1$fitted.values, m1$residuals, 
     xlab = "Fitted Values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values Plot",
     ylim = c(-50, 60))
abline(h = 0, lty = 2, col = 'red')
```

## Multicollinearity (1)

```{r}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "1,2|1,2,3"
# VIF test 
library(car)
vif(m1)
```

TYPE HIER: one thing we can do to check for multicolinearity is calculating VIFs...

## Multicollinearity (2) {auto-animate="true"}

``` r
# correlation matrix
```

## Multicollinearity (2) {auto-animate="true"}

``` r
# correlation matrix
cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")])
```

```{r, echo=FALSE}
cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")])
```

## Multicollinearity (2) {auto-animate="true"}

``` r
# correlation matrix
cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")])
```

```{r, echo=FALSE}
cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")])
```

TYPE HIER: one thing we can do to check for multicolinearity is making a correlation matrix...

## Multicollinearity (3) {auto-animate="true"}

``` r
# eigenvalues and condition number 
```

## Multicollinearity (3) {auto-animate="true"}

``` r
# eigenvalues and condition number 
eigen(cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")]))$values
kappa(model.matrix(m1))
```

```{r, echo=FALSE}
eigen(cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")]))$values
kappa(model.matrix(m1))
```

## Multicollinearity (3) {auto-animate="true"}

``` r
# eigenvalues and condition number 
eigen(cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")]))$values
kappa(model.matrix(m1))
```

```{r, echo=FALSE}
eigen(cor(df_cleaned[c("covid", "neighbourhood_num", "roomtype_num", "accommodates", "price", "instant_bookable")]))$values
kappa(model.matrix(m1))
```

TYPE HIER: a last thing we can do to check for multicolinearity is calculating eigenvalues and condition number...

# Conclusion

## Conclusion

TYPE HIER: Type the conclusion here
